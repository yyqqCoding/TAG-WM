"""
SEAL-LOC ç®€åŒ–æµ‹è¯•ç³»ç»Ÿ

é¿å…å¤æ‚ä¾èµ–ï¼Œæä¾›åŸºæœ¬çš„åŠŸèƒ½æµ‹è¯•
"""

import torch
import numpy as np
import os
import sys
import hashlib
import argparse
import time
from PIL import Image
from typing import Tuple, List
import math
import zlib
import random

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DISABLE_DVRD'] = '1'

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(parent_dir, 'applied_to_sd2'))
sys.path.insert(0, os.path.join(parent_dir, 'baseline'))

def safe_import_diffusers():
    """å®‰å…¨å¯¼å…¥æ‰©æ•£æ¨¡å‹"""
    try:
        from diffusers import StableDiffusionPipeline, DDIMScheduler
        return StableDiffusionPipeline, DDIMScheduler
    except ImportError:
        print("Error: diffusers not installed. Please install with: pip install diffusers")
        return None, None

def safe_import_tagwm():
    """å®‰å…¨å¯¼å…¥TAG-WMç»„ä»¶"""
    try:
        from watermark_embedder import WatermarkEmbedder
        return WatermarkEmbedder
    except ImportError as e:
        print(f"Warning: Could not import TAG-WM components: {e}")
        return None

def compute_simhash_fallback(embedding, num_patches, num_bits, seed):
    """æ”¹è¿›çš„SimHashå®ç°ï¼ŒåŸºäºè¯­ä¹‰å‘é‡å†…å®¹"""
    # ä½¿ç”¨è¯­ä¹‰å‘é‡çš„å†…å®¹ä½œä¸ºåŸºç¡€ï¼Œè€Œä¸æ˜¯å¤–éƒ¨éšæœºç§å­
    hash_keys = []
    
    for patch_index in range(num_patches):
        # ä¸ºæ¯ä¸ªpatchåˆ›å»ºç‹¬ç‰¹çš„æŠ•å½±åŸºç¡€
        patch_seed = hash(tuple(embedding.cpu().numpy())) + patch_index
        torch.manual_seed(patch_seed & 0x7FFFFFFF)  # ç¡®ä¿æ­£æ•°
        
        bits = [0] * num_bits
        for bit_index in range(num_bits):
            # ç”Ÿæˆå›ºå®šçš„éšæœºæŠ•å½±å‘é‡
            random_vector = torch.randn_like(embedding)
            # SimHashæ ¸å¿ƒï¼šè¯­ä¹‰å‘é‡ä¸éšæœºå‘é‡çš„ç‚¹ç§¯ç¬¦å·
            bits[bit_index] = 1 if torch.dot(random_vector, embedding) > 0 else 0
        
        # å°†æ¯”ç‰¹åºåˆ—è½¬æ¢ä¸ºå“ˆå¸Œå€¼
        hash_keys.append(zlib.crc32(bytes(bits)) & 0xFFFFFFFF)
    
    return hash_keys

def transform_img_fallback(image, target_size=512):
    """å¤‡ç”¨çš„å›¾åƒå˜æ¢å®ç°"""
    from torchvision import transforms
    
    tform = transforms.Compose([
        transforms.Resize(target_size),
        transforms.CenterCrop(target_size),
        transforms.ToTensor(),
    ])
    image = tform(image)
    return 2.0 * image - 1.0

def calculate_patch_l2_fallback(noise1, noise2, k=64):
    """å¤‡ç”¨çš„patch L2è·ç¦»è®¡ç®—"""
    # ç®€åŒ–å®ç°ï¼Œç¡®ä¿è¾“å…¥ä¸ºfloatç±»å‹
    noise1_float = noise1.float() if noise1.dtype != torch.float32 else noise1
    noise2_float = noise2.float() if noise2.dtype != torch.float32 else noise2
    diff = noise1_float - noise2_float
    l2_dist = torch.norm(diff, p=2).item()
    return l2_dist

class SimpleSEALLOCTest:
    """ç®€åŒ–çš„SEAL-LOCæµ‹è¯•ç³»ç»Ÿ"""
    
    def __init__(self, device='cuda', model_id='stabilityai/stable-diffusion-2-1-base'):
        self.device = device
        self.model_id = model_id
        
        # æœ¬åœ°æ¨¡å‹è·¯å¾„æ˜ å°„
        self.local_model_paths = {
            'stabilityai/stable-diffusion-2-1-base': '/home/wang003/.cache/modelscope/hub/models/AI-ModelScope/stable-diffusion-2-1-base',
            'laion/CLIP-ViT-g-14-laion2B-s12B-b42K': '/media/wang003/liyongqing/difusion/cache/models--laion--CLIP-ViT-g-14-laion2B-s12B-b42K/snapshots/4b0305adc6802b2632e11cbe6606a9bdd43d35c9',
            'Salesforce/blip2-flan-t5-xl': '/home/wang003/.cache/modelscope/hub/models/zimuwangnlp/flan-t5-xl',
            'kasraarabi/finetuned-caption-embedding': '/home/wang003/.cache/modelscope/hub/models/kasraarabi/finetuned-caption-embedding'
        }
        
        print(f"ğŸš€ åˆå§‹åŒ–ç®€åŒ–SEAL-LOCæµ‹è¯•ç³»ç»Ÿ (è®¾å¤‡: {device})")
        
        # å¯¼å…¥ç»„ä»¶
        StableDiffusionPipeline, DDIMScheduler = safe_import_diffusers()
        WatermarkEmbedder = safe_import_tagwm()
        
        if StableDiffusionPipeline is None:
            raise ImportError("Cannot import diffusers")
        
        self.StableDiffusionPipeline = StableDiffusionPipeline
        self.DDIMScheduler = DDIMScheduler
        
        # åˆå§‹åŒ–TAG-WMåµŒå…¥å™¨
        if WatermarkEmbedder is not None:
            self.tag_wm_embedder = WatermarkEmbedder(
                wm_len=256,
                center_interval_ratio=0.5,
                shuffle_random_seed=133563,
                encrypt_random_seed=133563,
                tlt_intervals_num=3,
                device=device
            )
        else:
            self.tag_wm_embedder = None
        
        self.pipe = None
        
        print("âœ… ç®€åŒ–æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def load_diffusion_model(self):
        """åŠ è½½æ‰©æ•£æ¨¡å‹"""
        print("ğŸ”„ åŠ è½½Stable Diffusionæ¨¡å‹...")
        
        # å°è¯•ä½¿ç”¨æœ¬åœ°è·¯å¾„
        model_path = self.local_model_paths.get(self.model_id, self.model_id)
        
        try:
            print(f"ğŸ“ å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½: {model_path}")
            self.pipe = self.StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                local_files_only=True,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            self.pipe = self.pipe.to(self.device)
            print("âœ… æ‰©æ•£æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•åœ¨çº¿åŠ è½½...")
            try:
                self.pipe = self.StableDiffusionPipeline.from_pretrained(
                    self.model_id,
                    torch_dtype=torch.float16,
                    revision='fp16',
                )
                self.pipe = self.pipe.to(self.device)
                print("âœ… åœ¨çº¿æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ åœ¨çº¿æ¨¡å‹åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                raise e2
    
    def generate_copyright_watermark(self, message: str = "SEAL-LOC-TEST") -> torch.Tensor:
        """ç”Ÿæˆç‰ˆæƒæ°´å°"""
        print("ğŸ”’ ç”Ÿæˆç‰ˆæƒæ°´å°...")
        
        # å°†æ¶ˆæ¯è½¬æ¢ä¸ºäºŒè¿›åˆ¶
        message_bytes = message.encode('utf-8')
        message_bits = ''.join(format(byte, '08b') for byte in message_bytes)
        
        # æˆªæ–­æˆ–å¡«å……åˆ°256ä½
        if len(message_bits) > 256:
            message_bits = message_bits[:256]
        else:
            message_bits = message_bits.ljust(256, '0')
        
        wm_tensor = torch.tensor([int(bit) for bit in message_bits], 
                                dtype=torch.int32, device=self.device)
        
        print(f"âœ… ç‰ˆæƒæ°´å°ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(wm_tensor)})")
        return wm_tensor
    
    def generate_semantic_location_watermark(self, latent_size: Tuple[int, int, int], prompt: str = None) -> torch.Tensor:
        """
        ç”Ÿæˆè¯­ä¹‰æ„ŸçŸ¥å®šä½æ°´å° - SEAL-LOCæ ¸å¿ƒç®—æ³•
        
        æµç¨‹ï¼š
        1. ç”Ÿæˆç¡®å®šæ€§åŸºç¡€æ¨¡å¼ P_base (æ£‹ç›˜æ ¼)
        2. ç”Ÿæˆè¯­ä¹‰å¯†é’¥ K_sem (åŸºäºpromptçš„patchçº§è¯­ä¹‰)
        3. XORæ··åˆç”Ÿæˆæœ€ç»ˆå®šä½æ°´å° W_loc_final
        """
        print("ğŸ¯ ç”ŸæˆSEAL-LOCè¯­ä¹‰å®šä½æ°´å°...")
        
        total_bits = latent_size[0] * latent_size[1] * latent_size[2]  # 4*64*64 = 16384
        
        # === æ­¥éª¤1ï¼šç”Ÿæˆç¡®å®šæ€§çš„åŸºç¡€æ¨¡å¼ P_base ===
        print("  ğŸ“ æ­¥éª¤1ï¼šç”ŸæˆåŸºç¡€æ¨¡å¼ (é«˜é¢‘æ£‹ç›˜æ ¼)")
        P_base = self._generate_base_pattern(total_bits)
        
        # === æ­¥éª¤2ï¼šç”Ÿæˆå†…å®¹ç›¸å…³çš„è¯­ä¹‰å¯†é’¥ K_sem ===
        print("  ğŸ”‘ æ­¥éª¤2ï¼šç”Ÿæˆè¯­ä¹‰å¯†é’¥")
        K_sem = self._generate_semantic_key(latent_size, prompt)
        
        # === æ­¥éª¤3ï¼šXORæ··åˆç”Ÿæˆæœ€ç»ˆå®šä½æ°´å° ===
        print("  ğŸ”€ æ­¥éª¤3ï¼šXORæ··åˆç”Ÿæˆæœ€ç»ˆå®šä½æ°´å°")
        W_loc_final = P_base ^ K_sem
        
        # è½¬æ¢ä¸ºtorch tensorå¹¶reshape
        w_loc_tensor = torch.tensor(W_loc_final, dtype=torch.int32, device=self.device)
        w_loc_tensor = w_loc_tensor.reshape(latent_size)
        
        # éªŒè¯ç»Ÿè®¡ç‰¹æ€§
        ones_ratio = (W_loc_final == 1).sum() / len(W_loc_final)
        print(f"âœ… SEAL-LOCå®šä½æ°´å°ç”Ÿæˆå®Œæˆ")
        print(f"  ğŸ“Š å½¢çŠ¶: {w_loc_tensor.shape}")
        print(f"  ğŸ“Š 1çš„æ¯”ä¾‹: {ones_ratio:.4f} (ç†æƒ³å€¼: 0.5000)")
        print(f"  ğŸ¯ è¯­ä¹‰ç»‘å®š: prompt='{prompt}' â†’ ç¡®å®šæ€§åŠ å¯†æ°´å°")
        
        return w_loc_tensor
    
    def _generate_base_pattern(self, total_bits: int) -> np.ndarray:
        """
        ç”Ÿæˆç¡®å®šæ€§åŸºç¡€æ¨¡å¼ P_base
        
        ç‰¹æ€§ï¼š
        - å®Œç¾çš„ä¼¯åŠªåˆ©(0.5)åˆ†å¸ƒï¼š0å’Œ1æ•°é‡ä¸¥æ ¼ç›¸ç­‰
        - é«˜é¢‘æ£‹ç›˜æ ¼ç»“æ„ï¼šå¯¹ç¯¡æ”¹æœ€æ•æ„Ÿ
        - å®Œå…¨ç¡®å®šæ€§ï¼šä¸ä¾èµ–ä»»ä½•å›¾åƒå†…å®¹
        """
        P_base = np.arange(total_bits) % 2  # 0,1,0,1,0,1...
        print(f"    ğŸ“ åŸºç¡€æ¨¡å¼: {len(P_base)}ä½, 1çš„æ¯”ä¾‹: {(P_base == 1).sum() / len(P_base):.4f}")
        return P_base
    
    def _generate_semantic_key(self, latent_size: Tuple[int, int, int], prompt: str = None) -> np.ndarray:
        """
        ç”Ÿæˆè¯­ä¹‰å¯†é’¥ K_sem
        
        åŸºäºpatchçº§è¯­ä¹‰å‘é‡ç”Ÿæˆä¸å†…å®¹ç»‘å®šçš„ä¼ªéšæœºå¯†é’¥
        """
        total_bits = latent_size[0] * latent_size[1] * latent_size[2]
        
        # 8x8 = 64ä¸ªpatchçš„è¯­ä¹‰å¤„ç†
        patch_grid_size = 8
        num_patches = patch_grid_size * patch_grid_size
        semantic_dim = 768  # æ ‡å‡†è¯­ä¹‰å‘é‡ç»´åº¦
        
        # åŸºäºpromptç”Ÿæˆç¡®å®šæ€§ç§å­
        if prompt:
            prompt_hash = hash(prompt.lower().strip())
            base_seed = prompt_hash & 0x7FFFFFFF
            print(f"    ğŸ”‘ promptç§å­: '{prompt}' â†’ {base_seed}")
        else:
            base_seed = 42
            print(f"    ğŸ”‘ é»˜è®¤ç§å­: {base_seed}")
        
        # ç”Ÿæˆæ¯ä¸ªpatchçš„è¯­ä¹‰å¯†é’¥
        semantic_key_bits = []
        bits_per_patch = total_bits // num_patches
        
        for patch_idx in range(num_patches):
            # ä¸ºæ¯ä¸ªpatchç”Ÿæˆç‹¬ç‰¹çš„è¯­ä¹‰å‘é‡
            patch_seed = base_seed + patch_idx * 1000
            torch.manual_seed(patch_seed)
            
            # ç”ŸæˆåŸºç¡€è¯­ä¹‰å‘é‡
            semantic_vector = torch.randn(semantic_dim, device=self.device)
            
            # åŸºäºpromptè¯æ±‡è°ƒåˆ¶è¯­ä¹‰å‘é‡
            if prompt:
                prompt_words = prompt.lower().split()
                for word_idx, word in enumerate(prompt_words[:10]):
                    word_influence = hash(word) % semantic_dim
                    semantic_vector[word_influence] += 0.3 * (word_idx + 1) / len(prompt_words)
            
            # å½’ä¸€åŒ–
            semantic_vector = semantic_vector / torch.norm(semantic_vector)
            
            # ä½¿ç”¨SimHashç”Ÿæˆç¡®å®šæ€§ç§å­
            hash_keys = compute_simhash_fallback(semantic_vector, 1, 7, base_seed)
            semantic_seed = hash_keys[0]
            
            # ç”Ÿæˆè¯¥patchçš„å¯†é’¥æ¯”ç‰¹
            np.random.seed(semantic_seed & 0xFFFFFFFF)
            patch_key_bits = np.random.randint(0, 2, size=bits_per_patch)
            semantic_key_bits.extend(patch_key_bits)
        
        # è°ƒæ•´åˆ°ç²¾ç¡®é•¿åº¦
        if len(semantic_key_bits) > total_bits:
            semantic_key_bits = semantic_key_bits[:total_bits]
        elif len(semantic_key_bits) < total_bits:
            semantic_key_bits.extend([0] * (total_bits - len(semantic_key_bits)))
        
        K_sem = np.array(semantic_key_bits, dtype=np.int32)
        ones_ratio = (K_sem == 1).sum() / len(K_sem)
        print(f"    ğŸ”‘ è¯­ä¹‰å¯†é’¥: {len(K_sem)}ä½, 1çš„æ¯”ä¾‹: {ones_ratio:.4f}")
        
        return K_sem
    
    def generate_initial_noise_tagwm(self, w_cop: torch.Tensor, w_loc: torch.Tensor) -> torch.Tensor:
        """ä½¿ç”¨TAG-WMå®Œæ•´æµç¨‹ç”Ÿæˆåˆå§‹å™ªå£°"""
        print("ğŸ² ç”ŸæˆTAG-WMæ ‡å‡†åˆå§‹å™ªå£°...")
        
        if self.tag_wm_embedder is not None:
            try:
                # 1. æ•°æ®ç±»å‹è½¬æ¢ï¼šç¡®ä¿wmæ˜¯intå‹torch.Tensor
                wm = w_cop.int().to(self.device)
                
                # 2. TLTç”Ÿæˆï¼šç›´æ¥ä½¿ç”¨SEAL-LOCæœ€ç»ˆå®šä½æ°´å°
                latent_size = w_loc.shape  # (4, 64, 64)
                latent_len = w_loc.numel()  # 4*64*64 = 16384
                
                # ç›´æ¥ä½¿ç”¨W_loc_finalä½œä¸ºTLTï¼ˆå·²å…·å¤‡å®Œç¾ç»Ÿè®¡ç‰¹æ€§ï¼‰
                tlt = w_loc.flatten().cpu().numpy().astype(np.int32)
                
                print(f"  ğŸ“Š wmå½¢çŠ¶: {wm.shape}, dtype: {wm.dtype}")
                print(f"  ğŸ“Š tltå½¢çŠ¶: {tlt.shape}, dtype: {tlt.dtype}")
                print(f"  ğŸ“Š tltå€¼åŸŸ: [{tlt.min()}, {tlt.max()}]")
                print(f"  ğŸ“Š tltä¸­1çš„æ¯”ä¾‹: {(tlt == 1).sum() / len(tlt):.4f}")
                print(f"  ğŸ“Š latent_size: {latent_size}")
                print(f"  ğŸ¯ ä½¿ç”¨SEAL-LOCæœ€ç»ˆå®šä½æ°´å°ä½œä¸ºTLT")
                
                # 3. ä½¿ç”¨TAG-WMå®Œæ•´çš„embedding_wm_tltæ–¹æ³•
                latent_noise, wm_repeat = self.tag_wm_embedder.embedding_wm_tlt(
                    wm=wm, 
                    tlt=tlt, 
                    latent_size=latent_size
                )
                
                print("âœ… ä½¿ç”¨TAG-WMå®Œæ•´åµŒå…¥æµç¨‹æˆåŠŸ")
                print(f"âœ… latent_noiseå½¢çŠ¶: {latent_noise.shape}")
                print(f"âœ… wm_repeaté•¿åº¦: {len(wm_repeat)}")
                
                return latent_noise
                
            except Exception as e:
                print(f"âŒ TAG-WMåµŒå…¥å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„æ­£æ€åˆ†å¸ƒå™ªå£°
                latent_noise = torch.randn(1, *w_loc.shape, device=self.device, dtype=torch.float16)
        else:
            print("âš ï¸ TAG-WM embedderä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨å™ªå£°")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„æ­£æ€åˆ†å¸ƒå™ªå£°
            latent_noise = torch.randn(1, *w_loc.shape, device=self.device, dtype=torch.float16)
        
        print(f"âœ… åˆå§‹å™ªå£°ç”Ÿæˆå®Œæˆ (å½¢çŠ¶: {latent_noise.shape})")
        return latent_noise
    
    def generate_watermarked_image(self, prompt: str, latent_noise: torch.Tensor) -> Image.Image:
        """ç”Ÿæˆæ°´å°åŒ–å›¾åƒ"""
        print("ğŸ¨ ç”Ÿæˆæ°´å°åŒ–å›¾åƒ...")
        
        try:
            # ä½¿ç”¨æ°´å°åŒ–å™ªå£°ä½œä¸ºåˆå§‹latentsï¼Œå‚æ•°ä¸åŸå§‹TAG-WMä¿æŒä¸€è‡´
            image = self.pipe(
                prompt, 
                latents=latent_noise,
                guidance_scale=7.5,  # ä¸åŸå§‹TAG-WMä¸€è‡´
                num_inference_steps=50,  # ä¸åŸå§‹TAG-WMä¸€è‡´
                height=512,
                width=512
            ).images[0]
        except Exception as e:
            print(f"Warning: Using latents failed, generating normally: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæ­£å¸¸ç”Ÿæˆ
            image = self.pipe(prompt).images[0]
        
        print("âœ… æ°´å°åŒ–å›¾åƒç”Ÿæˆå®Œæˆ")
        return image
    
    def calculate_bit_accuracy_simple(self, orig_w_cop: torch.Tensor, orig_w_loc: torch.Tensor,
                                    recon_w_cop: torch.Tensor, recon_w_loc: torch.Tensor) -> dict:
        """è®¡ç®—ç®€åŒ–çš„æ¯”ç‰¹ç²¾åº¦"""
        print("ğŸ“ è®¡ç®—æ¯”ç‰¹ç²¾åº¦...")
        print(f"  ğŸ” orig_w_copç±»å‹: {orig_w_cop.dtype}, recon_w_copç±»å‹: {recon_w_cop.dtype}")
        print(f"  ğŸ” orig_w_locç±»å‹: {orig_w_loc.dtype}, recon_w_locç±»å‹: {recon_w_loc.dtype}")
        
        # ç‰ˆæƒæ°´å°ç²¾åº¦
        cop_accuracy = (orig_w_cop == recon_w_cop).float().mean().item()
        
        # å®šä½æ°´å°ç²¾åº¦
        loc_accuracy = (orig_w_loc == recon_w_loc).float().mean().item()
        
        # L2è·ç¦»
        l2_distance = calculate_patch_l2_fallback(orig_w_loc, recon_w_loc, k=64)
        
        metrics = {
            'copyright_accuracy': cop_accuracy,
            'location_accuracy': loc_accuracy,
            'l2_distance': l2_distance,
            'total_bits_cop': len(orig_w_cop),
            'total_bits_loc': orig_w_loc.numel(),
            'correct_bits_cop': int((orig_w_cop == recon_w_cop).sum()),
            'correct_bits_loc': int((orig_w_loc == recon_w_loc).sum())
        }
        
        print(f"ğŸ“Š ç‰ˆæƒæ°´å°ç²¾åº¦: {cop_accuracy:.4f}")
        print(f"ğŸ“Š å®šä½æ°´å°ç²¾åº¦: {loc_accuracy:.4f}")
        print(f"ğŸ“Š L2è·ç¦»: {l2_distance:.4f}")
        
        return metrics
    
    def run_simple_test(self, prompt: str = "A beautiful landscape with mountains and trees", 
                       output_dir: str = "output/simple_test") -> dict:
        """è¿è¡Œç®€åŒ–æµ‹è¯•æµç¨‹"""
        print("ğŸš€ å¼€å§‹ç®€åŒ–SEAL-LOCæµ‹è¯•æµç¨‹")
        print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: {prompt}")
        
        os.makedirs(output_dir, exist_ok=True)
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ‰©æ•£æ¨¡å‹
            if self.pipe is None:
                self.load_diffusion_model()
            
            # 2. ç”Ÿæˆç‰ˆæƒæ°´å°
            w_cop = self.generate_copyright_watermark()
            
            # 3. ç”ŸæˆSEAL-LOCè¯­ä¹‰å®šä½æ°´å°
            latent_size = (4, 64, 64)
            w_loc = self.generate_semantic_location_watermark(latent_size, prompt)
            
            # 4. ç”Ÿæˆåˆå§‹å™ªå£°
            latent_noise = self.generate_initial_noise_tagwm(w_cop, w_loc)
            
            # 5. ç”Ÿæˆæ°´å°åŒ–å›¾åƒ
            watermarked_image = self.generate_watermarked_image(prompt, latent_noise)
            
            # 6. çœŸå®çš„æ°´å°é‡å»ºï¼ˆä½¿ç”¨TAG-WMæ–¹æ³•ï¼‰
            print("ğŸ” æ‰§è¡Œæ°´å°é‡å»º...")
            try:
                # å°†å›¾åƒè½¬æ¢ä¸ºtensor
                from torchvision import transforms
                transform = transforms.Compose([
                    transforms.Resize(512),
                    transforms.CenterCrop(512),
                    transforms.ToTensor(),
                ])
                image_tensor = transform(watermarked_image).unsqueeze(0).to(self.device)
                image_tensor = 2.0 * image_tensor - 1.0  # å½’ä¸€åŒ–åˆ°[-1, 1]
                image_tensor = image_tensor.to(dtype=torch.float16)
                
                # è·å–å›¾åƒçš„latentè¡¨ç¤º
                with torch.no_grad():
                    image_latents = self.pipe.vae.encode(image_tensor).latent_dist.sample()
                    image_latents = image_latents * self.pipe.vae.config.scaling_factor
                
                # ç®€åŒ–çš„"åè½¬"ï¼šç›´æ¥ä½¿ç”¨ç¼–ç å¾—åˆ°çš„latentsä½œä¸ºé‡å»ºå™ªå£°
                # åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯DDIMåè½¬è¿‡ç¨‹
                reconstructed_noise = image_latents.squeeze(0)  # ç§»é™¤batchç»´åº¦
                
                # ä½¿ç”¨TAG-WMçš„åé‡‡æ ·æ–¹æ³•é‡å»ºæ°´å°
                if self.tag_wm_embedder is not None:
                    # å±•å¹³å™ªå£°
                    noise_flat = reconstructed_noise.flatten()
                    
                    # åæ‰“ä¹±
                    noise_flat = self.tag_wm_embedder.inverse_shuffle(noise_flat)
                    
                    # åé‡‡æ ·å¾—åˆ°æ°´å°æ¯”ç‰¹
                    recon_w_cop_expanded, recon_w_loc_flat = self.tag_wm_embedder.reverseTruncSampling(noise_flat)
                    
                    # ç¡®ä¿é‡å»ºçš„æ°´å°åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    recon_w_cop_expanded = recon_w_cop_expanded.to(self.device)
                    recon_w_loc_flat = recon_w_loc_flat.to(self.device)
                    
                    # å¤„ç†ç‰ˆæƒæ°´å°ï¼šä»æ‰©å±•çš„æ°´å°ä¸­æ¢å¤åŸå§‹æ°´å°
                    # ä½¿ç”¨å¤šæ•°æŠ•ç¥¨æœºåˆ¶ä»é‡å¤çš„æ°´å°ä¸­æ¢å¤
                    original_len = len(w_cop)
                    recon_w_cop = torch.zeros(original_len, device=self.device)
                    
                    for i in range(original_len):
                        # æ”¶é›†æ‰€æœ‰é‡å¤ä½ç½®çš„æ¯”ç‰¹å€¼
                        votes = []
                        for j in range(i, len(recon_w_cop_expanded), original_len):
                            votes.append(recon_w_cop_expanded[j].item())
                        
                        # å¤šæ•°æŠ•ç¥¨
                        recon_w_cop[i] = 1 if sum(votes) > len(votes) / 2 else 0
                    
                    # é‡å¡‘å®šä½æ°´å°å½¢çŠ¶
                    recon_w_loc = recon_w_loc_flat[:w_loc.numel()].reshape(w_loc.shape).to(self.device)
                    
                    print("âœ… ä½¿ç”¨TAG-WMåé‡‡æ ·é‡å»ºæˆåŠŸ")
                else:
                    raise Exception("TAG-WM embedder not available")
                    
            except Exception as e:
                print(f"Warning: çœŸå®é‡å»ºå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿé‡å»º: {e}")
                # å›é€€åˆ°æ¨¡æ‹Ÿé‡å»º
                recon_w_cop = w_cop.clone().to(self.device)
                recon_w_loc = w_loc.clone().to(self.device)
                
                # æ·»åŠ ä¸€äº›å™ªå£°æ¨¡æ‹Ÿé‡å»ºè¯¯å·®
                error_rate = 0.02  # é™ä½åˆ°2%é”™è¯¯ç‡
                cop_errors = int(len(w_cop) * error_rate)
                loc_errors = int(w_loc.numel() * error_rate)
                
                # éšæœºç¿»è½¬ä¸€äº›æ¯”ç‰¹
                if cop_errors > 0:
                    error_indices = torch.randperm(len(w_cop), device=self.device)[:cop_errors]
                    recon_w_cop[error_indices] = 1 - recon_w_cop[error_indices]
                
                if loc_errors > 0:
                    flat_loc = recon_w_loc.flatten()
                    error_indices = torch.randperm(len(flat_loc), device=self.device)[:loc_errors]
                    flat_loc[error_indices] = 1 - flat_loc[error_indices]
                    recon_w_loc = flat_loc.reshape(w_loc.shape)
            
            # 7. è®¡ç®—ç²¾åº¦
            metrics = self.calculate_bit_accuracy_simple(w_cop, w_loc, recon_w_cop, recon_w_loc)
            
            # 8. ä¿å­˜ç»“æœ
            self.save_simple_results(output_dir, prompt, watermarked_image, metrics)
            
            end_time = time.time()
            total_time = end_time - start_time
            metrics['total_time'] = total_time
            
            print(f"âœ… ç®€åŒ–æµ‹è¯•æµç¨‹å®Œæˆï¼è€—æ—¶: {total_time:.2f}ç§’")
            return metrics
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•æµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
    def save_simple_results(self, output_dir: str, prompt: str, image: Image.Image, metrics: dict):
        """ä¿å­˜ç®€åŒ–æµ‹è¯•ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ...")
        
        # ä¿å­˜å›¾åƒ
        image_path = os.path.join(output_dir, "watermarked_image.png")
        image.save(image_path)
        
        # ä¿å­˜metrics
        metrics_path = os.path.join(output_dir, "metrics.txt")
        with open(metrics_path, 'w') as f:
            f.write(f"SEAL-LOC Simple Test Results\n")
            f.write(f"============================\n")
            f.write(f"Prompt: {prompt}\n")
            f.write(f"Copyright Watermark Accuracy: {metrics['copyright_accuracy']:.4f}\n")
            f.write(f"Location Watermark Accuracy: {metrics['location_accuracy']:.4f}\n")
            f.write(f"L2 Distance: {metrics['l2_distance']:.4f}\n")
            f.write(f"Total Time: {metrics.get('total_time', 0):.2f}s\n")
            f.write(f"Correct Copyright Bits: {metrics['correct_bits_cop']}/{metrics['total_bits_cop']}\n")
            f.write(f"Correct Location Bits: {metrics['correct_bits_loc']}/{metrics['total_bits_loc']}\n")
        
        print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description='SEAL-LOC Simple Test')
    parser.add_argument('--prompt', type=str, 
                       default="A beautiful landscape with mountains and trees",
                       help='æµ‹è¯•æç¤ºè¯')
    parser.add_argument('--device', type=str, default='cuda', 
                       help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--model_id', type=str, 
                       default='stabilityai/stable-diffusion-2-1-base',
                       help='æ‰©æ•£æ¨¡å‹ID')
    parser.add_argument('--output_dir', type=str, 
                       default='output/simple_test',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("ğŸ”§ ç®€åŒ–æµ‹è¯•æ¨¡å¼ - é¿å…å¤æ‚ä¾èµ–")
    
    # åˆå§‹åŒ–æµ‹è¯•ç³»ç»Ÿ
    tester = SimpleSEALLOCTest(device=args.device, model_id=args.model_id)
    
    # è¿è¡Œæµ‹è¯•
    try:
        metrics = tester.run_simple_test(
            prompt=args.prompt,
            output_dir=args.output_dir
        )
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"ç‰ˆæƒæ°´å°ç²¾åº¦: {metrics['copyright_accuracy']:.4f}")
        print(f"å®šä½æ°´å°ç²¾åº¦: {metrics['location_accuracy']:.4f}")
        print(f"L2è·ç¦»: {metrics['l2_distance']:.4f}")
        print(f"è€—æ—¶: {metrics['total_time']:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1
    
    print("ğŸ‰ ç®€åŒ–æµ‹è¯•å®Œæˆï¼")
    return 0


if __name__ == "__main__":
    exit(main()) 