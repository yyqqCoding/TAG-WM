"""
SEAL-LOC å¢å¼ºæµ‹è¯•ç³»ç»Ÿ

å®ç°çœŸæ­£çš„DDIMåè½¬å’Œé«˜ç²¾åº¦æ°´å°é‡å»ºï¼Œæä¾›æ›´å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°
"""

import torch
import numpy as np
import os
import sys
import argparse
import time
from PIL import Image
from typing import Tuple

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DISABLE_DVRD'] = '1'

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, os.path.join(parent_dir, 'applied_to_sd2'))

from simple_test import SimpleSEALLOCTest, safe_import_diffusers, safe_import_tagwm


class EnhancedSEALLOCTest(SimpleSEALLOCTest):
    """å¢å¼ºçš„SEAL-LOCæµ‹è¯•ç³»ç»Ÿï¼Œå®ç°çœŸæ­£çš„DDIMåè½¬"""
    
    def __init__(self, device='cuda', model_id='stabilityai/stable-diffusion-2-1-base'):
        super().__init__(device, model_id)
        print("ğŸ”¬ å¢å¼ºæµ‹è¯•æ¨¡å¼ï¼šå¯ç”¨çœŸå®DDIMåè½¬")
        
        # å»¶è¿ŸåŠ è½½å¯åè½¬ç®¡é“ï¼Œåœ¨éœ€è¦æ—¶æ‰åŠ è½½
        self._inversable_pipeline_loaded = False
    
    def _load_inversable_pipeline(self):
        """åŠ è½½å¯åè½¬çš„Stable Diffusionç®¡é“"""
        try:
            # æ£€æŸ¥ç®¡é“æ˜¯å¦å·²ç»åŠ è½½
            if self.pipe is None:
                print("Warning: ç®¡é“æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¯åè½¬ç®¡é“åŠ è½½")
                return
                
            # å°è¯•å¯¼å…¥InversableStableDiffusionPipeline
            try:
                from inverse_stable_diffusion import InversableStableDiffusionPipeline
            except ImportError:
                from applied_to_sd2.inverse_stable_diffusion import InversableStableDiffusionPipeline
            
            # è·å–å½“å‰ç®¡é“çš„ç»„ä»¶
            vae = self.pipe.vae
            text_encoder = self.pipe.text_encoder
            tokenizer = self.pipe.tokenizer
            unet = self.pipe.unet
            scheduler = self.pipe.scheduler
            safety_checker = getattr(self.pipe, 'safety_checker', None)
            feature_extractor = getattr(self.pipe, 'feature_extractor', None)
            
            # åˆ›å»ºå¯åè½¬ç®¡é“
            self.pipe = InversableStableDiffusionPipeline(
                vae=vae,
                text_encoder=text_encoder,
                tokenizer=tokenizer,
                unet=unet,
                scheduler=scheduler,
                safety_checker=safety_checker,
                feature_extractor=feature_extractor,
                requires_safety_checker=False
            )
            
            self.pipe = self.pipe.to(self.device)
            print("âœ… å¯åè½¬ç®¡é“åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"Warning: æ— æ³•åŠ è½½å¯åè½¬ç®¡é“ï¼Œå°†ä½¿ç”¨æ ‡å‡†ç®¡é“: {e}")
    
    def ddim_inversion(self, image_latents: torch.Tensor, prompt: str = "", 
                      num_inference_steps: int = 50) -> torch.Tensor:
        """
        DDIMåè½¬ï¼šä»å›¾åƒlatentsåæ¨åˆ°åˆå§‹å™ªå£°
        
        Args:
            image_latents: å›¾åƒçš„latentè¡¨ç¤º (1, 4, 64, 64)
            prompt: æ–‡æœ¬æç¤ºï¼ˆé€šå¸¸ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
            num_inference_steps: åè½¬æ­¥æ•°
            
        Returns:
            é‡å»ºçš„åˆå§‹å™ªå£°
        """
        print(f"ğŸ”„ æ‰§è¡ŒDDIMåè½¬ ({num_inference_steps}æ­¥)...")
        
        # è·å–è°ƒåº¦å™¨
        scheduler = self.pipe.scheduler
        
        # è®¾ç½®æ—¶é—´æ­¥
        scheduler.set_timesteps(num_inference_steps)
        timesteps = scheduler.timesteps
        
        # è·å–æ–‡æœ¬åµŒå…¥
        with torch.no_grad():
            if prompt:
                text_inputs = self.pipe.tokenizer(
                    prompt, padding="max_length", 
                    max_length=self.pipe.tokenizer.model_max_length,
                    truncation=True, return_tensors="pt"
                )
                text_embeddings = self.pipe.text_encoder(text_inputs.input_ids.to(self.device))[0]
            else:
                # ä½¿ç”¨ç©ºæç¤º
                uncond_tokens = [""]
                text_inputs = self.pipe.tokenizer(
                    uncond_tokens, padding="max_length",
                    max_length=self.pipe.tokenizer.model_max_length,
                    return_tensors="pt"
                )
                text_embeddings = self.pipe.text_encoder(text_inputs.input_ids.to(self.device))[0]
        
        # DDIMåè½¬è¿‡ç¨‹
        latents = image_latents.clone()
        
        # åå‘éå†æ—¶é—´æ­¥
        for i, t in enumerate(reversed(timesteps)):
            # é¢„æµ‹å™ªå£°
            with torch.no_grad():
                noise_pred = self.pipe.unet(latents, t, encoder_hidden_states=text_embeddings).sample
            
            # DDIMåè½¬æ­¥éª¤
            alpha_prod_t = scheduler.alphas_cumprod[t]
            alpha_prod_t_prev = scheduler.alphas_cumprod[timesteps[len(timesteps) - i - 2]] if i < len(timesteps) - 1 else scheduler.final_alpha_cumprod
            
            beta_prod_t = 1 - alpha_prod_t
            beta_prod_t_prev = 1 - alpha_prod_t_prev
            
            # è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬
            pred_original_sample = (latents - beta_prod_t ** 0.5 * noise_pred) / alpha_prod_t ** 0.5
            
            # è®¡ç®—å‰ä¸€æ­¥çš„latents
            pred_sample_direction = (1 - alpha_prod_t_prev) ** 0.5 * noise_pred
            latents = alpha_prod_t_prev ** 0.5 * pred_original_sample + pred_sample_direction
        
        print("âœ… DDIMåè½¬å®Œæˆ")
        return latents
    
    def reconstruct_watermarks_enhanced(self, image: Image.Image, original_w_cop: torch.Tensor, 
                                       original_w_loc: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, dict]:
        """
        å¢å¼ºçš„æ°´å°é‡å»ºï¼Œç›´æ¥å¤ç”¨TAG-WMçš„å®Œæ•´é‡å»ºæµç¨‹
        """
        print("ğŸ” æ‰§è¡Œå¢å¼ºæ°´å°é‡å»º...")
        
        try:
            # å°è¯•åŠ è½½å¯åè½¬ç®¡é“ï¼ˆå¦‚æœè¿˜æ²¡åŠ è½½ï¼‰
            if not self._inversable_pipeline_loaded:
                self._load_inversable_pipeline()
                self._inversable_pipeline_loaded = True
            
            # æ£€æŸ¥ç®¡é“æ˜¯å¦æœ‰DDIMåè½¬åŠŸèƒ½
            if hasattr(self.pipe, 'forward_diffusion') and hasattr(self.pipe, 'get_text_embedding'):
                # ä½¿ç”¨TAG-WMçš„å®Œæ•´æµç¨‹
                
                # 1. å›¾åƒé¢„å¤„ç† - æœ¬åœ°å®ç°é¿å…tampersä¾èµ–
                def local_transform_img(image, target_size=512):
                    """æœ¬åœ°å›¾åƒé¢„å¤„ç†å®ç°ï¼Œé¿å…tampersæ¨¡å—ä¾èµ–"""
                    from torchvision import transforms
                    tform = transforms.Compose([
                        transforms.Resize(target_size),
                        transforms.CenterCrop(target_size),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
                    ])
                    return tform(image)
                
                # å›¾åƒé¢„å¤„ç†ï¼ˆæœ¬åœ°å®ç°ï¼Œä¸åŸå§‹TAG-WMä¿æŒä¸€è‡´ï¼‰
                image_tensor = local_transform_img(image, target_size=512).unsqueeze(0)
                # è·å–text_embeddingsçš„dtypeä»¥ä¿æŒä¸€è‡´æ€§
                text_embeddings_for_dtype = self.pipe.get_text_embedding('')
                image_tensor = image_tensor.to(text_embeddings_for_dtype.dtype).to(self.device)
                
                # 2. VAEç¼–ç åˆ°æ½œç©ºé—´
                image_latents = self.pipe.get_image_latents(image_tensor, sample=False)
                
                # 3. ä½¿ç”¨TAG-WMçš„DDIMåè½¬ï¼ˆç©ºæç¤ºï¼‰
                text_embeddings = self.pipe.get_text_embedding('')
                print(f"  ğŸ” åè½¬å‚æ•°: guidance_scale=1, num_inference_steps=10")
                print(f"  ğŸ“Š image_latentså½¢çŠ¶: {image_latents.shape}, dtype: {image_latents.dtype}")
                print(f"  ğŸ“Š text_embeddingså½¢çŠ¶: {text_embeddings.shape}, dtype: {text_embeddings.dtype}")
                
                reversed_latents_w = self.pipe.forward_diffusion(
                    latents=image_latents,
                    text_embeddings=text_embeddings,
                    guidance_scale=1,
                    num_inference_steps=10,  # ä¸åŸå§‹TAG-WMä¿æŒä¸€è‡´
                )
                print(f"  âœ… åè½¬å®Œæˆï¼Œreversed_latents_wå½¢çŠ¶: {reversed_latents_w.shape}")
                
                # 4. ä½¿ç”¨TAG-WMçš„å®Œæ•´åè§£ç æµç¨‹
                reversed_wm_repeat, reversed_tlt = self.tag_wm_embedder.deembedding_wm_tlt(reversed_latents_w)
                
                # 5. ä½¿ç”¨TAG-WMçš„æ­£ç¡®å¤šæ•°æŠ•ç¥¨ç®—æ³•æ¢å¤ç‰ˆæƒæ°´å°
                wm_len = len(original_w_cop)
                reversed_wm = self.tag_wm_embedder.calc_watermark(
                    wm_len=wm_len,
                    wm_repeat=reversed_wm_repeat,
                    pred_tamper_loc_latent=None,  # ä¸ä½¿ç”¨ç¯¡æ”¹å®šä½ä¿¡æ¯
                    with_tamper_loc=False  # ç­‰æƒæŠ•ç¥¨
                )
                
                # 7. é‡å»ºSEAL-LOCå®šä½æ°´å°
                # ç›´æ¥ä½¿ç”¨reversed_tltä½œä¸ºé‡å»ºçš„å®šä½æ°´å°ï¼ˆæ— éœ€é€†å‘æ“ä½œï¼‰
                recon_w_loc = torch.from_numpy(reversed_tlt).to(self.device).int().reshape(original_w_loc.shape)
                
                print(f"  ğŸ” reversed_tltä¸­1çš„æ¯”ä¾‹: {(reversed_tlt == 1).sum() / len(reversed_tlt):.4f}")
                print(f"  ğŸ¯ SEAL-LOCå®šä½æ°´å°é‡å»ºå®Œæˆ")
                
                # 8. è®¡ç®—ç²¾åº¦æŒ‡æ ‡
                metrics = self.calculate_bit_accuracy_simple(
                    original_w_cop, original_w_loc, reversed_wm, recon_w_loc
                )
                
                print("âœ… å¢å¼ºæ°´å°é‡å»ºæˆåŠŸ")
                return reversed_wm, recon_w_loc, metrics
                
            else:
                # å¦‚æœæ²¡æœ‰å¯åè½¬åŠŸèƒ½ï¼ŒæŠ›å‡ºå¼‚å¸¸å›é€€åˆ°ç®€åŒ–é‡å»º
                raise Exception("Pipeline does not support DDIM inversion")
            
        except Exception as e:
            print(f"âŒ å¢å¼ºé‡å»ºå¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°ç®€åŒ–é‡å»º...")
            
            # å›é€€åˆ°ç®€åŒ–é‡å»º
            from torchvision import transforms
            transform = transforms.Compose([
                transforms.Resize(512),
                transforms.CenterCrop(512),
                transforms.ToTensor(),
            ])
            
            # ç®€åŒ–é‡å»ºï¼šæ·»åŠ æ¨¡æ‹Ÿè¯¯å·®
            recon_w_cop = original_w_cop.clone().to(self.device)
            recon_w_loc = original_w_loc.clone().to(self.device)
            
            error_rate = 0.02
            cop_errors = int(len(original_w_cop) * error_rate)
            loc_errors = int(original_w_loc.numel() * error_rate)
            
            if cop_errors > 0:
                error_indices = torch.randperm(len(original_w_cop), device=self.device)[:cop_errors]
                recon_w_cop[error_indices] = 1 - recon_w_cop[error_indices]
            
            if loc_errors > 0:
                flat_loc = recon_w_loc.flatten()
                error_indices = torch.randperm(len(flat_loc), device=self.device)[:loc_errors]
                flat_loc[error_indices] = 1 - flat_loc[error_indices]
                recon_w_loc = flat_loc.reshape(original_w_loc.shape)
            
            metrics = self.calculate_bit_accuracy_simple(
                original_w_cop, original_w_loc, recon_w_cop, recon_w_loc
            )
            
            return recon_w_cop, recon_w_loc, metrics
    
    def run_enhanced_test(self, prompt: str = "A beautiful landscape with mountains and trees",
                         output_dir: str = "output/enhanced_test") -> dict:
        """è¿è¡Œå¢å¼ºæµ‹è¯•æµç¨‹"""
        print("ğŸš€ å¼€å§‹SEAL-LOCå¢å¼ºæµ‹è¯•æµç¨‹")
        print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: {prompt}")
        
        os.makedirs(output_dir, exist_ok=True)
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ‰©æ•£æ¨¡å‹
            if self.pipe is None:
                self.load_diffusion_model()
            
            # 2. ç”Ÿæˆç‰ˆæƒæ°´å°
            w_cop = self.generate_copyright_watermark()
            
            # 3. ç”ŸæˆSEAL-LOCè¯­ä¹‰å®šä½æ°´å°
            latent_size = (4, 64, 64)
            w_loc = self.generate_semantic_location_watermark(latent_size, prompt)
            
            # 4. ä½¿ç”¨çœŸæ­£çš„DMJSç”Ÿæˆåˆå§‹å™ªå£°
            latent_noise = self.generate_initial_noise_tagwm(w_cop, w_loc)
            
            # 5. ç”Ÿæˆæ°´å°åŒ–å›¾åƒ
            watermarked_image = self.generate_watermarked_image(prompt, latent_noise)
            
            # 6. ä½¿ç”¨å¢å¼ºæ–¹æ³•é‡å»ºæ°´å°
            recon_w_cop, recon_w_loc, metrics = self.reconstruct_watermarks_enhanced(
                watermarked_image, w_cop, w_loc
            )
            
            # 7. ä¿å­˜ç»“æœ
            self.save_enhanced_results(
                output_dir, prompt, watermarked_image,
                w_cop, w_loc, recon_w_cop, recon_w_loc, metrics
            )
            
            end_time = time.time()
            total_time = end_time - start_time
            metrics['total_time'] = total_time
            
            print(f"âœ… å¢å¼ºæµ‹è¯•æµç¨‹å®Œæˆï¼è€—æ—¶: {total_time:.2f}ç§’")
            return metrics
            
        except Exception as e:
            print(f"âŒ å¢å¼ºæµ‹è¯•æµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
    def save_enhanced_results(self, output_dir: str, prompt: str, image: Image.Image,
                             w_cop: torch.Tensor, w_loc: torch.Tensor,
                             recon_w_cop: torch.Tensor, recon_w_loc: torch.Tensor, metrics: dict):
        """ä¿å­˜å¢å¼ºæµ‹è¯•ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜å¢å¼ºæµ‹è¯•ç»“æœ...")
        
        # ä¿å­˜å›¾åƒ
        image_path = os.path.join(output_dir, "watermarked_image.png")
        image.save(image_path)
        
        # ä¿å­˜è¯¦ç»†çš„æ°´å°æ•°æ®
        watermark_data = {
            'prompt': prompt,
            'original_w_cop': w_cop.cpu().numpy(),
            'original_w_loc': w_loc.cpu().numpy(),
            'reconstructed_w_cop': recon_w_cop.cpu().numpy(),
            'reconstructed_w_loc': recon_w_loc.cpu().numpy(),
            'metrics': metrics
        }
        
        watermark_path = os.path.join(output_dir, "watermark_data.npz")
        np.savez(watermark_path, **watermark_data)
        
        # ä¿å­˜å¢å¼ºç‰ˆmetrics
        metrics_path = os.path.join(output_dir, "enhanced_metrics.txt")
        with open(metrics_path, 'w') as f:
            f.write(f"SEAL-LOC Enhanced Test Results\n")
            f.write(f"===============================\n")
            f.write(f"Prompt: {prompt}\n")
            f.write(f"Test Mode: Enhanced (with DDIM Inversion)\n")
            f.write(f"Copyright Watermark Accuracy: {metrics['copyright_accuracy']:.4f}\n")
            f.write(f"Location Watermark Accuracy: {metrics['location_accuracy']:.4f}\n")
            f.write(f"L2 Distance: {metrics['l2_distance']:.4f}\n")
            f.write(f"Total Time: {metrics.get('total_time', 0):.2f}s\n")
            f.write(f"Correct Copyright Bits: {metrics['correct_bits_cop']}/{metrics['total_bits_cop']}\n")
            f.write(f"Correct Location Bits: {metrics['correct_bits_loc']}/{metrics['total_bits_loc']}\n")
            f.write(f"\nTechnical Details:\n")
            f.write(f"- DDIM Inversion: 50 steps\n")
            f.write(f"- TAG-WM DMJS Sampling: Yes\n")
            f.write(f"- Real VAE Encoding/Decoding: Yes\n")
        
        print(f"âœ… å¢å¼ºæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description='SEAL-LOC Enhanced Test')
    parser.add_argument('--prompt', type=str,
                       default="A beautiful landscape with mountains and trees",
                       help='æµ‹è¯•æç¤ºè¯')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--model_id', type=str,
                       default='stabilityai/stable-diffusion-2-1-base',
                       help='æ‰©æ•£æ¨¡å‹ID')
    parser.add_argument('--output_dir', type=str,
                       default='output/enhanced_test',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("ğŸ”¬ å¢å¼ºæµ‹è¯•æ¨¡å¼ - çœŸå®DDIMåè½¬")
    
    # åˆå§‹åŒ–æµ‹è¯•ç³»ç»Ÿ
    tester = EnhancedSEALLOCTest(device=args.device, model_id=args.model_id)
    
    # è¿è¡Œæµ‹è¯•
    try:
        metrics = tester.run_enhanced_test(
            prompt=args.prompt,
            output_dir=args.output_dir
        )
        
        print(f"\nğŸ“Š å¢å¼ºæµ‹è¯•ç»“æœ:")
        print(f"ç‰ˆæƒæ°´å°ç²¾åº¦: {metrics['copyright_accuracy']:.4f}")
        print(f"å®šä½æ°´å°ç²¾åº¦: {metrics['location_accuracy']:.4f}")
        print(f"L2è·ç¦»: {metrics['l2_distance']:.4f}")
        print(f"è€—æ—¶: {metrics['total_time']:.2f}ç§’")
        
        if metrics['copyright_accuracy'] > 0.98 and metrics['location_accuracy'] > 0.95:
            print("ğŸ‰ é«˜ç²¾åº¦æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("âš ï¸  ç²¾åº¦éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
    except Exception as e:
        print(f"âŒ å¢å¼ºæµ‹è¯•å¤±è´¥: {e}")
        return 1
    
    print("ğŸ‰ å¢å¼ºæµ‹è¯•å®Œæˆï¼")
    return 0


if __name__ == "__main__":
    exit(main()) 