"""
SEAL-LOC: Semantic-Aware Localization Watermark Embedder

åŸºäºè¯­ä¹‰çš„å®šä½æ°´å°åµŒå…¥å™¨ï¼Œé›†æˆSEALçš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ä¸TAG-WMçš„åŒæ°´å°æ¶æ„ã€‚
æ ¸å¿ƒç‰¹æ€§ï¼š
- ç»§æ‰¿TAG-WMçš„åŒæ°´å°æ¶æ„å’ŒDMJSç®—æ³•
- å®ç°é€è¡¥ä¸è¯­ä¹‰ç‰¹å¾æå–
- åŠ¨æ€ç”Ÿæˆä¸å†…å®¹ç»‘å®šçš„è¯­ä¹‰å®šä½æ°´å°
- ä¿æŒä¸åŸæœ‰ç³»ç»Ÿçš„å®Œå…¨å…¼å®¹æ€§
"""

import torch
import torch.nn as nn
import numpy as np
import os
import math
import hashlib
from typing import Tuple, List, Optional
from PIL import Image
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from sentence_transformers import SentenceTransformer
from model_config import get_model_path, check_model_exists

# å¯¼å…¥TAG-WMçš„æ ¸å¿ƒç»„ä»¶
import sys
sys.path.append('../applied_to_sd2')

# ä¸´æ—¶ç¦ç”¨DVRDå¯¼å…¥ä»¥é¿å…ä¾èµ–é—®é¢˜
import os
os.environ['DISABLE_DVRD'] = '1'
from watermark_embedder import WatermarkEmbedder

# å¯¼å…¥æœ¬åœ°çš„å›¾åƒæè¿°ç”Ÿæˆå‡½æ•°
from caption_utils import generate_caption


class SEALLOCEmbedder(WatermarkEmbedder):
    """
    è¯­ä¹‰åŒºåŸŸæ„ŸçŸ¥å®šä½æ°´å°åµŒå…¥å™¨ (SEAL-LOC)
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    - å°†å®šä½æ°´å°ä»å›ºå®šæ¨¡æ¿(TLT)å‡çº§ä¸ºåŠ¨æ€è¯­ä¹‰ç»‘å®šæ¨¡æ¿(W_loc^S)
    - æ¯ä¸ªpatchçš„æ°´å°æ¨¡å¼ä¸å…¶è¯­ä¹‰å†…å®¹å¯†ç å­¦çº§åˆ«ç»‘å®š
    - ä¿æŒTAG-WMçš„ç‰ˆæƒæ°´å°(W_cop)å’ŒDMJSé‡‡æ ·æœºåˆ¶ä¸å˜
    """
    
    def __init__(self, 
                 # ç»§æ‰¿TAG-WMçš„æ‰€æœ‰å‚æ•°
                 wm_len=256,
                 center_interval_ratio=0.5,
                 shuffle_random_seed=133563,
                 encrypt_random_seed=133563,
                 tlt_intervals_num=3,  # ä½¿ç”¨ä¸‰åŒºé—´ç­–ç•¥
                 fpr=1e-6,
                 user_number=1000000,
                 optimize_tamper_loc_method=None,
                 DVRD_checkpoint_path=None,
                 DVRD_train_size=512,
                 device='cuda',
                 # SEAL-LOCç‰¹æœ‰å‚æ•°
                 patch_grid_size=8,  # 8x8ç½‘æ ¼ï¼Œå…±64ä¸ªpatch
                 semantic_vector_dim=768,  # SentenceTransformerè¾“å‡ºç»´åº¦
                 simhash_bits=7,  # SimHashæ¯”ç‰¹æ•°
                 semantic_maps_dir='watermarkLOC/semantic_maps/',  # è¯­ä¹‰åœ°å›¾å­˜å‚¨è·¯å¾„
                 vlm_model_name='blip2-flan-t5-xl',  # VLMæ¨¡å‹
                 sentence_model_name='kasraarabi/finetuned-caption-embedding',  # è¯­ä¹‰ç¼–ç æ¨¡å‹
                 ):
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super(SEALLOCEmbedder, self).__init__(
            wm_len=wm_len,
            center_interval_ratio=center_interval_ratio,
            shuffle_random_seed=shuffle_random_seed,
            encrypt_random_seed=encrypt_random_seed,
            tlt_intervals_num=tlt_intervals_num,
            fpr=fpr,
            user_number=user_number,
            optimize_tamper_loc_method=optimize_tamper_loc_method,
            DVRD_checkpoint_path=DVRD_checkpoint_path,
            DVRD_train_size=DVRD_train_size,
            device=device
        )
        
        # SEAL-LOCç‰¹æœ‰é…ç½®
        self.patch_grid_size = patch_grid_size
        self.num_patches = patch_grid_size * patch_grid_size  # 64ä¸ªpatch
        self.semantic_vector_dim = semantic_vector_dim
        self.simhash_bits = simhash_bits
        self.semantic_maps_dir = semantic_maps_dir
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        os.makedirs(semantic_maps_dir, exist_ok=True)
        
        # åˆå§‹åŒ–VLMå’Œè¯­ä¹‰ç¼–ç æ¨¡å‹
        model_path = get_model_path(vlm_model_name)
        print(f"  - Loading VLM model: {vlm_model_name} from {model_path}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not check_model_exists(model_path):
            print(f"  âš ï¸ Model path does not exist: {model_path}")
            print("  è¯·ç¡®ä¿å·²å°†æ¨¡å‹ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„")
        
        # å°è¯•è§£å†³BLIP-2 tokenizeré—®é¢˜çš„å¤šç§æ–¹æ³•
        try:
            # æ–¹æ³•1: ä»æœ¬åœ°è·¯å¾„åŠ è½½
            self.vlm_processor = Blip2Processor.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True
            )
            self.vlm_model = Blip2ForConditionalGeneration.from_pretrained(
                model_path, 
                torch_dtype=torch.float16,
                trust_remote_code=True,
                local_files_only=True
            ).to(device)
            print("  âœ… VLM loaded successfully from local path")
        except Exception as e1:
            print(f"  âš ï¸ Local loading failed: {e1}")
            try:
                # æ–¹æ³•2: ä½¿ç”¨è¾ƒå°çš„BLIP-2æ¨¡å‹
                fallback_model_path = get_model_path("blip2-opt-2.7b")
                print(f"  ğŸ”„ Trying fallback model: {fallback_model_path}")
                self.vlm_processor = Blip2Processor.from_pretrained(
                    fallback_model_path,
                    trust_remote_code=True,
                    local_files_only=True
                )
                self.vlm_model = Blip2ForConditionalGeneration.from_pretrained(
                    fallback_model_path, 
                    torch_dtype=torch.float16,
                    trust_remote_code=True,
                    local_files_only=True
                ).to(device)
                print("  âœ… VLM loaded successfully (fallback model)")
            except Exception as e2:
                print(f"  âŒ All VLM loading methods failed: {e2}")
                print("  è¯·ç¡®ä¿BLIP-2æ¨¡å‹å·²æ­£ç¡®ä¸‹è½½åˆ°æœ¬åœ°è·¯å¾„")
                raise RuntimeError("æ— æ³•åŠ è½½BLIP-2æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        
        # åˆå§‹åŒ–SentenceTransformerï¼ˆæ”¯æŒæœ¬åœ°è·¯å¾„ï¼‰
        sentence_model_path = get_model_path(sentence_model_name)
        print(f"  - Loading SentenceTransformer: {sentence_model_name} from {sentence_model_path}")
        
        try:
            # å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½
            self.sentence_model = SentenceTransformer(sentence_model_path).to(device)
            print("  âœ… SentenceTransformer loaded successfully from local path")
        except Exception as e:
            print(f"  âš ï¸ Local SentenceTransformer loading failed: {e}")
            try:
                # å›é€€åˆ°åœ¨çº¿åŠ è½½
                self.sentence_model = SentenceTransformer(sentence_model_name).to(device)
                print("  âœ… SentenceTransformer loaded successfully from online")
            except Exception as e2:
                print(f"  âŒ All SentenceTransformer loading methods failed: {e2}")
                raise RuntimeError("æ— æ³•åŠ è½½SentenceTransformeræ¨¡å‹")
        
        print(f"SEAL-LOC Embedder initialized:")
        print(f"  - Patch grid: {patch_grid_size}x{patch_grid_size} = {self.num_patches} patches")
        print(f"  - Semantic vector dim: {semantic_vector_dim}")
        print(f"  - SimHash bits: {simhash_bits}")
        print(f"  - Semantic maps storage: {semantic_maps_dir}")
    
    def generate_proxy_image(self, prompt: str, pipe=None, num_inference_steps=20) -> Image.Image:
        """
        é˜¶æ®µä¸€ï¼šä»£ç†ç”Ÿæˆä¸åˆå§‹è¡¨ç¤º
        ç”Ÿæˆæ— æ°´å°çš„ä»£ç†å›¾åƒï¼Œç”¨äºåç»­è¯­ä¹‰æå–
        """
        if pipe is not None:
            with torch.no_grad():
                proxy_image = pipe(prompt).images[0]
        else:
            # ä½¿ç”¨å†…ç½®çš„VLMç”Ÿæˆç®€å•çš„ä»£ç†å›¾åƒ
            from diffusers import StableDiffusionPipeline
            temp_pipe = StableDiffusionPipeline.from_pretrained(
                "stabilityai/stable-diffusion-2-1",
                torch_dtype=torch.float16
            ).to(self.device)
            with torch.no_grad():
                proxy_image = temp_pipe(prompt, num_inference_steps=num_inference_steps).images[0]
            del temp_pipe  # é‡Šæ”¾å†…å­˜
        return proxy_image
    
    def extract_patch_semantics(self, proxy_image: Image.Image, latent_size: Tuple[int, int, int]) -> List[torch.Tensor]:
        """
        é˜¶æ®µäºŒï¼šé€è¡¥ä¸è¯­ä¹‰ç‰¹å¾æå–
        
        Args:
            proxy_image: ä»£ç†å›¾åƒ (512x512 RGB)
            latent_size: æ½œç©ºé—´å°ºå¯¸ (C, H, W) = (4, 64, 64)
            
        Returns:
            List of semantic vectors, one for each patch
        """
        _, latent_h, latent_w = latent_size
        patch_size_latent = latent_h // self.patch_grid_size  # 64 // 8 = 8
        patch_size_image = 512 // self.patch_grid_size  # 512 // 8 = 64
        
        semantic_vectors = []
        
        print(f"Extracting semantics for {self.num_patches} patches...")
        
        for i in range(self.patch_grid_size):
            for j in range(self.patch_grid_size):
                # è®¡ç®—patchåœ¨å›¾åƒç©ºé—´çš„ä½ç½®
                y_start = i * patch_size_image
                y_end = (i + 1) * patch_size_image
                x_start = j * patch_size_image
                x_end = (j + 1) * patch_size_image
                
                # è£å‰ªå¯¹åº”çš„å›¾åƒåŒºåŸŸ
                patch_image = proxy_image.crop((x_start, y_start, x_end, y_end))
                
                # ä½¿ç”¨VLMç”Ÿæˆè¯¥patchçš„æè¿°
                patch_caption = generate_caption(
                    patch_image, 
                    self.vlm_processor, 
                    self.vlm_model, 
                    device=self.device
                )
                
                # å°†æè¿°è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡
                semantic_vector = self.sentence_model.encode(
                    patch_caption, 
                    convert_to_tensor=True
                ).to(self.device)
                
                # å½’ä¸€åŒ–è¯­ä¹‰å‘é‡
                semantic_vector = semantic_vector / torch.norm(semantic_vector)
                
                semantic_vectors.append(semantic_vector)
        
        print(f"Semantic extraction completed. Generated {len(semantic_vectors)} vectors.")
        return semantic_vectors
    
    def generate_dynamic_semantic_watermark(self, semantic_vectors: List[torch.Tensor], latent_size: Tuple[int, int, int]) -> torch.Tensor:
        """
        é˜¶æ®µä¸‰ï¼šåŠ¨æ€è¯­ä¹‰å®šä½æ°´å°ç”Ÿæˆ
        
        ä¸ºæ¯ä¸ªpatchåŸºäºå…¶è¯­ä¹‰å‘é‡ç”Ÿæˆç‹¬ç«‹çš„æ°´å°æ¨¡å¼ï¼Œ
        ç„¶åæ‹¼æ¥æˆå®Œæ•´çš„W_loc^S
        
        Args:
            semantic_vectors: 64ä¸ªè¯­ä¹‰å‘é‡çš„åˆ—è¡¨
            latent_size: æ½œç©ºé—´å°ºå¯¸ (4, 64, 64)
            
        Returns:
            W_loc^S: åŠ¨æ€è¯­ä¹‰å®šä½æ°´å° (torch.Tensor)
        """
        latent_len = np.prod(latent_size)  # 4 * 64 * 64 = 16384
        patch_latent_len = latent_len // self.num_patches  # 16384 // 64 = 256
        
        w_loc_s = torch.zeros(latent_len, device=self.device)
        
        print(f"Generating dynamic semantic watermark for {self.num_patches} patches...")
        
        for patch_idx, semantic_vector in enumerate(semantic_vectors):
            # ä½¿ç”¨SimHashç”Ÿæˆè¯¥patchçš„å“ˆå¸Œå€¼
            hash_value = self._compute_patch_simhash(semantic_vector, patch_idx)
            
            # åŸºäºå“ˆå¸Œå€¼ç”Ÿæˆè¯¥patchçš„æ°´å°æ¯”ç‰¹æµ
            patch_watermark_bits = self._generate_patch_watermark_bits(hash_value, patch_latent_len)
            
            # å°†è¯¥patchçš„æ°´å°å¡«å…¥å¯¹åº”ä½ç½®
            start_idx = patch_idx * patch_latent_len
            end_idx = (patch_idx + 1) * patch_latent_len
            w_loc_s[start_idx:end_idx] = torch.from_numpy(patch_watermark_bits).float().to(self.device)
        
        print("Dynamic semantic watermark generation completed.")
        return w_loc_s
    
    def _compute_patch_simhash(self, semantic_vector: torch.Tensor, patch_idx: int) -> int:
        """
        ä¸ºå•ä¸ªpatchè®¡ç®—SimHashå€¼
        
        Args:
            semantic_vector: è¯¥patchçš„è¯­ä¹‰å‘é‡ (768ç»´)
            patch_idx: patchç´¢å¼• (0-63)
            
        Returns:
            æ•´æ•°å“ˆå¸Œå€¼ï¼Œç”¨ä½œPRNGç§å­
        """
        # å¯¼å…¥æœ¬åœ°çš„simhashå‡½æ•°
        from simhash_utils import simhash_single_patch
        
        # ä½¿ç”¨ä¼˜åŒ–çš„å•patch SimHashå‡½æ•°
        base_hash = simhash_single_patch(
            semantic_vector, 
            num_bits=self.simhash_bits,
            seed=42 + patch_idx  # ä¸ºæ¯ä¸ªpatchä½¿ç”¨ä¸åŒç§å­
        )
        
        # ç»“åˆpatchç´¢å¼•å’Œå…¨å±€saltç”Ÿæˆæœ€ç»ˆç§å­
        # å¤ç”¨TAG-WMçš„ç¡®å®šæ€§éšæœºæ•°ç”Ÿæˆæœºåˆ¶
        combined_seed = f"{base_hash}_{patch_idx}_{self.encrypt_random_seed}"
        
        # ä½¿ç”¨å“ˆå¸Œå‡½æ•°ç”Ÿæˆæœ€ç»ˆçš„æ•´æ•°ç§å­
        final_seed = int(hashlib.md5(combined_seed.encode()).hexdigest()[:8], 16)
        
        return final_seed
    
    def _generate_patch_watermark_bits(self, seed: int, length: int) -> np.ndarray:
        """
        åŸºäºç§å­ç”Ÿæˆç¡®å®šæ€§çš„æ°´å°æ¯”ç‰¹æµ
        
        Args:
            seed: SimHashç”Ÿæˆçš„ç§å­
            length: éœ€è¦ç”Ÿæˆçš„æ¯”ç‰¹æ•°é‡
            
        Returns:
            äºŒè¿›åˆ¶æ¯”ç‰¹æ•°ç»„ (0/1)
        """
        # ä½¿ç”¨numpyçš„éšæœºæ•°ç”Ÿæˆå™¨ï¼Œç¡®ä¿ç¡®å®šæ€§
        rng = np.random.RandomState(seed)
        watermark_bits = rng.randint(0, 2, size=length, dtype=np.uint8)
        return watermark_bits
    
    def save_semantic_map(self, semantic_vectors: List[torch.Tensor], identifier: str):
        """
        ä¿å­˜åŸºå‡†çœŸå®è¯­ä¹‰åœ°å›¾åˆ°æœ¬åœ°æ–‡ä»¶
        
        Args:
            semantic_vectors: 64ä¸ªè¯­ä¹‰å‘é‡
            identifier: å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¦‚promptçš„hashï¼‰
        """
        # å°†è¯­ä¹‰å‘é‡åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡
        semantic_map = torch.stack(semantic_vectors)  # Shape: (64, 768)
        
        # ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
        save_path = os.path.join(self.semantic_maps_dir, f"semantic_map_{identifier}.pt")
        torch.save(semantic_map, save_path)
        
        print(f"Semantic map saved to: {save_path}")
        return save_path
    
    def load_semantic_map(self, identifier: str) -> Optional[List[torch.Tensor]]:
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½åŸºå‡†è¯­ä¹‰åœ°å›¾
        
        Args:
            identifier: å”¯ä¸€æ ‡è¯†ç¬¦
            
        Returns:
            è¯­ä¹‰å‘é‡åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None
        """
        load_path = os.path.join(self.semantic_maps_dir, f"semantic_map_{identifier}.pt")
        
        if not os.path.exists(load_path):
            return None
        
        try:
            semantic_map = torch.load(load_path, map_location=self.device)  # Shape: (64, 768)
            semantic_vectors = [semantic_map[i] for i in range(semantic_map.shape[0])]
            print(f"Semantic map loaded from: {load_path}")
            return semantic_vectors
        except Exception as e:
            print(f"Error loading semantic map: {e}")
            return None
    
    def text_to_bits(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ¯”ç‰¹åºåˆ—"""
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚ï¼Œç„¶åè½¬æ¢ä¸ºæ¯”ç‰¹
        text_bytes = text.encode('utf-8')
        bits = []
        for byte in text_bytes:
            for i in range(8):
                bits.append((byte >> (7-i)) & 1)
        return torch.tensor(bits, dtype=torch.float32, device=self.device)
    
    def embedding_seal_loc(self, prompt: str, wm: torch.Tensor, latent_size: Tuple[int, int, int], pipe=None) -> torch.Tensor:
        """
        SEAL-LOCå®Œæ•´åµŒå…¥æµç¨‹
        
        é›†æˆå…­ä¸ªé˜¶æ®µï¼š
        1. ä»£ç†ç”Ÿæˆä¸åˆå§‹è¡¨ç¤º
        2. é€è¡¥ä¸è¯­ä¹‰ç‰¹å¾æå–  
        3. åŠ¨æ€è¯­ä¹‰å®šä½æ°´å°ç”Ÿæˆ
        4. ç‰ˆæƒæ°´å°ç”Ÿæˆï¼ˆå¤ç”¨TAG-WMï¼‰
        5. åŒæ°´å°è”åˆé‡‡æ ·ï¼ˆDMJSï¼‰
        6. æ‰©æ•£ç”Ÿæˆä¸è§£ç 
        
        Args:
            prompt: ç”¨æˆ·æ–‡æœ¬æç¤ºè¯
            wm: ç‰ˆæƒæ°´å°æ¯”ç‰¹ä¸² (256ä½)
            latent_size: æ½œç©ºé—´å°ºå¯¸ (4, 64, 64)
            pipe: æ‰©æ•£æ¨¡å‹ç®¡çº¿ (å¯é€‰)
            
        Returns:
            latent_noise: æ°´å°åŒ–çš„åˆå§‹å™ªå£°
        """
        print("=== SEAL-LOC Watermark Embedding Pipeline ===")
        
        # é˜¶æ®µä¸€ï¼šç”Ÿæˆä»£ç†å›¾åƒ
        print("Stage 1: Proxy image generation...")
        if pipe is not None:
            proxy_image = self.generate_proxy_image(prompt, pipe)
        else:
            proxy_image = self.generate_proxy_image(prompt)
        
        # é˜¶æ®µäºŒï¼šé€è¡¥ä¸è¯­ä¹‰æå–
        print("Stage 2: Patch-wise semantic extraction...")
        semantic_vectors = self.extract_patch_semantics(proxy_image, latent_size)
        
        # ä¿å­˜åŸºå‡†è¯­ä¹‰åœ°å›¾
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:16]
        self.save_semantic_map(semantic_vectors, prompt_hash)
        
        # é˜¶æ®µä¸‰ï¼šåŠ¨æ€è¯­ä¹‰å®šä½æ°´å°ç”Ÿæˆ
        print("Stage 3: Dynamic semantic watermark generation...")
        w_loc_s = self.generate_dynamic_semantic_watermark(semantic_vectors, latent_size)
        
        # é˜¶æ®µå››ï¼šç‰ˆæƒæ°´å°ç”Ÿæˆï¼ˆå¤ç”¨TAG-WMé€»è¾‘ï¼‰
        print("Stage 4: Copyright watermark generation...")
        latent_len = np.prod(latent_size)
        tlt_dummy = torch.zeros(latent_len, device=self.device)  # å ä½ç¬¦ï¼Œå°†è¢«w_loc_sæ›¿ä»£
        w_cop = self._generate_copyright_watermark(wm, latent_len)
        
        # é˜¶æ®µäº”ï¼šåŒæ°´å°è”åˆé‡‡æ ·ï¼ˆDMJSï¼‰
        print("Stage 5: Dual watermark joint sampling (DMJS)...")
        latent_noise, wm_repeat = self.embedding_wm_tlt(w_cop, w_loc_s, latent_size)
        
        print("=== SEAL-LOC Embedding Completed ===")
        return latent_noise
    
    def _generate_copyright_watermark(self, wm: torch.Tensor, latent_len: int) -> torch.Tensor:
        """
        ç”Ÿæˆç‰ˆæƒæ°´å°ï¼ˆå¤ç”¨TAG-WMçš„é€»è¾‘ï¼‰
        
        Args:
            wm: åŸå§‹æ°´å°æ¯”ç‰¹ä¸²
            latent_len: æ½œç©ºé—´æ€»é•¿åº¦
            
        Returns:
            æ‰©å±•å¹¶åŠ å¯†åçš„ç‰ˆæƒæ°´å°
        """
        # é‡å¤å±•å¼€æ°´å°ä»¥è¦†ç›–æ½œç©ºé—´
        wm_len = wm.shape[0]
        wm_times = int(latent_len // wm_len)
        remain_wm_len = latent_len % wm_len
        wm_repeat = torch.concat([wm.repeat(wm_times), wm[:remain_wm_len]], dim=0)
        
        # æµåŠ å¯†ï¼ˆå¯é€‰ï¼‰- ç¡®ä¿è¾“å…¥æ˜¯æ•´æ•°ç±»å‹
        wm_repeat_int = wm_repeat.cpu().numpy().astype(np.uint8)
        wm_repeat_encrypt = self.stream_key_encrypt(wm_repeat_int)
        
        return torch.from_numpy(wm_repeat_encrypt).float().to(self.device)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åˆå§‹åŒ–SEAL-LOCåµŒå…¥å™¨
    seal_loc = SEALLOCEmbedder(device=device)
    
    print("SEAL-LOC Embedder initialized successfully!")
    print(f"Number of patches: {seal_loc.num_patches}")
    print(f"Semantic vector dimension: {seal_loc.semantic_vector_dim}") 